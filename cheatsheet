{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30527,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Introduction #\nThis will be a personal guide for basic data processing before using it to train a model. It resumes the basic information given on the courses of: Data Cleaning, Feature Engineering and Intermediate Machine Learning. The data processing will consist on the following main steps:\n- **Data Preprocessing**, where we'll get the data clean and ready to use it.\n- **Feature Engineering**, where we'll try to get the most of our data by crating new features or transforming them.","metadata":{"_uuid":"62604194-6d12-48fe-b4ab-f0237553442a","_cell_guid":"951d085b-255a-474d-b1bb-60e689a73699","trusted":true}},{"cell_type":"markdown","source":"## Imports ##\nFirst of all is important to import every library that we're going to use and load our data.","metadata":{"_uuid":"af68ef52-7bc7-4b76-a2b3-1bd676aa2245","_cell_guid":"f5a99629-eb40-4a47-b4ea-fa28233c78c5","trusted":true}},{"cell_type":"code","source":"# Basic libraries\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom IPython.display import display\nfrom pandas.api.types import CategoricalDtype\n\n# Custom libraries for encoding, imputation, and other feature engineering techniques\nfrom category_encoders import MEstimateEncoder\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\nfrom sklearn.feature_selection import mutual_info_regression\nfrom sklearn.model_selection import KFold, cross_val_score\n\nfrom category_encoders import MEstimateEncoder\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\nfrom sklearn.feature_selection import mutual_info_regression\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom xgboost import XGBRegressor\n\n# Model library (XGBoost)\nfrom xgboost import XGBRegressor\n\n# Set Matplotlib defaults\nplt.style.use(\"seaborn-whitegrid\")\nplt.rc(\"figure\", autolayout=True)\nplt.rc(\n    \"axes\",\n    labelweight=\"bold\",\n    labelsize=\"large\",\n    titleweight=\"bold\",\n    titlesize=14,\n    titlepad=10,\n)","metadata":{"_uuid":"b5b6fa99-9c14-4001-979b-12f2f556d049","_cell_guid":"57fccd13-0906-408c-8ddf-abdcf12e80a9","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-08-29T14:21:56.402236Z","iopub.execute_input":"2023-08-29T14:21:56.402671Z","iopub.status.idle":"2023-08-29T14:21:58.667425Z","shell.execute_reply.started":"2023-08-29T14:21:56.402638Z","shell.execute_reply":"2023-08-29T14:21:58.663975Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Preprocessing #\nThe **Data Preprocessing** will consist on the following 4 steps:\n- **Load** the data from CSV files\n- **Clean** the data to fix any errors or inconsistencies\n- **Encode** the statistical data type (numeric, categorical)\n- **Impute** any missing values\n\nWe'll wrap all these steps up in a function, which will make easy for you to get a fresh dataframe whenever you need. After reading the CSV file, we'll apply three preprocessing steps, `clean`, `encode`, and `impute`, and then create the data splits: one (`df_train`) for training the model, and one (`df_test`) for making the predictions that you'll submit to the competition for scoring on the leaderboard.","metadata":{}},{"cell_type":"markdown","source":"## Data Loading ##\nWe'll use a single function to load our data to keep everything clean. This function will work when our dataset has train and test data already split.\n\nIf having trouble loading the data, you can check the tutorial on decoding data from other formats [here](https://www.kaggle.com/code/alexisbcook/character-encodings).","metadata":{"_uuid":"b72f35d4-041b-4a1c-bc39-af2da4e9c558","_cell_guid":"e4c17e4e-9c94-4073-a5cb-17e754e9585b","trusted":true}},{"cell_type":"code","source":"file_path = \"...\" # Introduce file path\n\ndef load_data():\n    # Read data\n    data_dir = Path(file_path)\n    df_train = pd.read_csv(data_dir / \"train.csv\", index_col=\"Id\")\n    df_test = pd.read_csv(data_dir / \"test.csv\", index_col=\"Id\")\n    # Merge the splits so we can process them together\n    df = pd.concat([df_train, df_test])\n    # Preprocessing\n    df = clean(df)\n    df = encode(df)\n    df = impute(df)\n    # Reform splits\n    df_train = df.loc[df_train.index, :]\n    df_test = df.loc[df_test.index, :]\n    return df_train, df_test","metadata":{"_uuid":"f2d8ed88-8064-44d2-9459-783349668335","_cell_guid":"caa94655-97d6-4ec0-8890-bc8e09b0caa2","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-08-29T14:21:58.669658Z","iopub.execute_input":"2023-08-29T14:21:58.670001Z","iopub.status.idle":"2023-08-29T14:21:58.679328Z","shell.execute_reply.started":"2023-08-29T14:21:58.669961Z","shell.execute_reply":"2023-08-29T14:21:58.677682Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Cleaning ##\nWhen cleaning the data it's important to look for this ways to get the data cleaner:\n- **Checking data is \"well\" written**, for example, names starting with numbers are awkard to work with.\n- **Scaling and normalizing** the data to make it fit between certain values, or simply make your data match the *Gaussian distribution* (which you can make using the [Box-Cox](https://en.wikipedia.org/wiki/Power_transform#Box%E2%80%93Cox_transformation) transofrmation with `from scipy import stats`.\n- **Checking data is in its proper format**, for example, if using dates check whether the columns that contain them are in the pandas date type. You can check pandas dtype cocumentation [here](https://pandas.pydata.org/pandas-docs/stable/user_guide/basics.html) and a dates cheatsheet [here](https://strftime.org/).\n- **Fixing incosistency on data** by checking unique values on various columns and making sure there are no typos. You can use fuzzy matching technique from the [fuzzywuzzy](https://github.com/seatgeek/fuzzywuzzy) package and checking this [tutorial](https://www.kaggle.com/code/alexisbcook/inconsistent-data-entry) if necessary.","metadata":{"_uuid":"b7a1a55c-047e-4cc8-a3a8-64c44b0c40b5","_cell_guid":"733d6afc-f5d2-4a65-a990-f8f91d3da982","trusted":true}},{"cell_type":"code","source":"# A fuzzy matching function to fix inconsistent data can look like this\nimport fuzzywuzzy\nfrom fuzzywuzzy import process\nimport charset_normalizer\n\ndef replace_matches_in_column(df, column, string_to_match, min_ratio):\n    # get a list of unique strings\n    strings = df[column].unique()\n    \n    # get the top 10 closest matches to our input string\n    matches = fuzzywuzzy.process.extract(string_to_match, strings, \n                                         limit=10, scorer=fuzzywuzzy.fuzz.token_sort_ratio)\n\n    # only get matches with a ratio > 90\n    close_matches = [matches[0] for matches in matches if matches[1] >= min_ratio]\n\n    # get the rows of all the close matches in our dataframe\n    rows_with_matches = df[column].isin(close_matches)\n\n    # replace all rows with close matches with the input matches \n    df.loc[rows_with_matches, column] = string_to_match\n    \n    # let us know the function's done\n    print(\"All done!\")","metadata":{"_uuid":"a0944e1b-423f-4509-8281-62810d614591","_cell_guid":"db158d92-daa9-42d0-98b1-fa2192a2fe65","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-08-29T14:21:58.680859Z","iopub.execute_input":"2023-08-29T14:21:58.681361Z","iopub.status.idle":"2023-08-29T14:21:58.812711Z","shell.execute_reply.started":"2023-08-29T14:21:58.681327Z","shell.execute_reply":"2023-08-29T14:21:58.811454Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Encoding ##\nThis will consist on **encoding categorical data** into columns that can be used by our model to learn. To do it, we can use various techniques: ordinal encoding, one-hot encoding, etc. For both of them you can use the `OrdinalEncoder` and `OneHotEncoder` from the `skelarn.preprocessing` module (see documentation [here](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html)).\n\nOn other hand, some categorical variables can be read as non categorical, which must be changed.","metadata":{}},{"cell_type":"code","source":"# A function to encode categorical variables can look\n# like this (this is an example from the bonus lesson\n# on the feature engineering course, which works on the\n# \"house-prices-advanced-regression-technique\" dataset).\n\n# The nominative (unordered) categorical features\nfeatures_nom = [\"MSSubClass\", \"MSZoning\", \"Street\", \"Alley\", \"LandContour\", \"LotConfig\", \"Neighborhood\", \"Condition1\", \"Condition2\", \"BldgType\", \"HouseStyle\", \"RoofStyle\", \"RoofMatl\", \"Exterior1st\", \"Exterior2nd\", \"MasVnrType\", \"Foundation\", \"Heating\", \"CentralAir\", \"GarageType\", \"MiscFeature\", \"SaleType\", \"SaleCondition\"]\n\n\n# The ordinal (ordered) categorical features \n\n# Pandas calls the categories \"levels\"\nfive_levels = [\"Po\", \"Fa\", \"TA\", \"Gd\", \"Ex\"]\nten_levels = list(range(10))\n\nordered_levels = {\n    \"OverallQual\": ten_levels,\n    \"OverallCond\": ten_levels,\n    \"ExterQual\": five_levels,\n    \"ExterCond\": five_levels,\n    \"BsmtQual\": five_levels,\n    \"BsmtCond\": five_levels,\n    \"HeatingQC\": five_levels,\n    \"KitchenQual\": five_levels,\n    \"FireplaceQu\": five_levels,\n    \"GarageQual\": five_levels,\n    \"GarageCond\": five_levels,\n    \"PoolQC\": five_levels,\n    \"LotShape\": [\"Reg\", \"IR1\", \"IR2\", \"IR3\"],\n    \"LandSlope\": [\"Sev\", \"Mod\", \"Gtl\"],\n    \"BsmtExposure\": [\"No\", \"Mn\", \"Av\", \"Gd\"],\n    \"BsmtFinType1\": [\"Unf\", \"LwQ\", \"Rec\", \"BLQ\", \"ALQ\", \"GLQ\"],\n    \"BsmtFinType2\": [\"Unf\", \"LwQ\", \"Rec\", \"BLQ\", \"ALQ\", \"GLQ\"],\n    \"Functional\": [\"Sal\", \"Sev\", \"Maj1\", \"Maj2\", \"Mod\", \"Min2\", \"Min1\", \"Typ\"],\n    \"GarageFinish\": [\"Unf\", \"RFn\", \"Fin\"],\n    \"PavedDrive\": [\"N\", \"P\", \"Y\"],\n    \"Utilities\": [\"NoSeWa\", \"NoSewr\", \"AllPub\"],\n    \"CentralAir\": [\"N\", \"Y\"],\n    \"Electrical\": [\"Mix\", \"FuseP\", \"FuseF\", \"FuseA\", \"SBrkr\"],\n    \"Fence\": [\"MnWw\", \"GdWo\", \"MnPrv\", \"GdPrv\"],\n}\n\n# Add a None level for missing values\nordered_levels = {key: [\"None\"] + value for key, value in\n                  ordered_levels.items()}\n\n\ndef encode(df):\n    # Nominal categories\n    for name in features_nom:\n        df[name] = df[name].astype(\"category\")\n        # Add a None category for missing values\n        if \"None\" not in df[name].cat.categories:\n            df[name].cat.add_categories(\"None\", inplace=True)\n    # Ordinal categories\n    for name, levels in ordered_levels.items():\n        df[name] = df[name].astype(CategoricalDtype(levels,\n                                                    ordered=True))\n    return df","metadata":{"execution":{"iopub.status.busy":"2023-08-29T14:21:58.815616Z","iopub.execute_input":"2023-08-29T14:21:58.815966Z","iopub.status.idle":"2023-08-29T14:21:58.831733Z","shell.execute_reply.started":"2023-08-29T14:21:58.815935Z","shell.execute_reply":"2023-08-29T14:21:58.830299Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Imputing ##\nThis will consist on **handling missing values** that can be identified with NaN, \"Unkown\", \"None\", etc. You can either drop them (not reccomendable), or fill them (impute) with various techniques like the fillna function on pandas (there are already various methods to do it, check out the [documentation](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.fillna.html)) or using the `SimpleImputer` from `sklearn` (check documentation [here](https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html)).","metadata":{}},{"cell_type":"code","source":"# The lines of code for imputing missing values with the SimpleImputer can look like this\nfrom sklearn.impute import SimpleImputer\n\ncols_with_missing = [col for col in X_train.columns\n                     if X_train[col].isnull().any()]\n\nX_train_plus = X_train.copy()\nX_test_plus = X_test.copy()\nfor col in cols_with_missing:\n    X_train_plus[col + '_was_missing'] = X_train_plus[col].isnull()\n    X_valid_plus[col + '_was_missing'] = X_test_plus[col].isnull()\n\n# Imputation\nmy_imputer = SimpleImputer()\nimputed_X_train_plus = pd.DataFrame(my_imputer.fit_transform(X_train_plus))\nimputed_X_test_plus = pd.DataFrame(my_imputer.transform(X_test_plus))\n\n# Imputation removed column names; put them back\nimputed_X_train_plus.columns = X_train_plus.columns\nimputed_X_test_plus.columns = X_test_plus.columns","metadata":{"execution":{"iopub.status.busy":"2023-08-29T14:21:58.833162Z","iopub.execute_input":"2023-08-29T14:21:58.833504Z","iopub.status.idle":"2023-08-29T14:21:59.452113Z","shell.execute_reply.started":"2023-08-29T14:21:58.833465Z","shell.execute_reply":"2023-08-29T14:21:59.450406Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature Engineering #\nThis will consist on the following steps:\n- **Take MI scores** to help us see which features have the strongest relationship with the target feature.\n- **Create features** using the relation between existing ones, this can be done with a lot of various techniques: using pandas to manually create features (with encoding, mathematical transforms, interactions, counts, break downs, group transforms, etc.), with k-means clustering, PCA, target encoding, and else.","metadata":{}},{"cell_type":"markdown","source":"## MI Scores ##\nThere's some things to bare in mind when interpreting the MI scores:\n- The least possible mutual information between quantities is 0.0. When MI is zero, the quantities are independent: neither can tell you anything about the other. Conversely, in theory there's no upper bound to what MI can be. In practice though values above 2.0 or so are uncommon. (Mutual information is a logarithmic quantity, so it increases very slowly.)\n- MI can help you to understand the relative potential of a feature as a predictor of the target, considered by itself.\n- It's possible for a feature to be very informative when interacting with other features, but not so informative all alone. MI can't detect interactions between features. It is a univariate metric.\n- The actual usefulness of a feature depends on the model you use it with. A feature is only useful to the extent that its relationship with the target is one your model can learn. Just because a feature has a high MI score doesn't mean your model will be able to do anything with that information. You may need to transform the feature first to expose the association.","metadata":{}},{"cell_type":"code","source":"# These are two functions that can be used to make and plot MI scores\n\ndef make_mi_scores(X, y):\n    X = X.copy()\n    for colname in X.select_dtypes([\"object\", \"category\"]):\n        X[colname], _ = X[colname].factorize()\n    # All discrete features should now have integer dtypes\n    discrete_features = [pd.api.types.is_integer_dtype(t) for t in X.dtypes]\n    mi_scores = mutual_info_regression(X, y, discrete_features=discrete_features, random_state=0)\n    mi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=X.columns)\n    mi_scores = mi_scores.sort_values(ascending=False)\n    return mi_scores\n\n\ndef plot_mi_scores(scores):\n    scores = scores.sort_values(ascending=True)\n    width = np.arange(len(scores))\n    ticks = list(scores.index)\n    plt.barh(width, scores)\n    plt.yticks(width, ticks)\n    plt.title(\"Mutual Information Scores\")","metadata":{"execution":{"iopub.status.busy":"2023-08-29T14:22:09.035923Z","iopub.execute_input":"2023-08-29T14:22:09.036360Z","iopub.status.idle":"2023-08-29T14:22:09.047755Z","shell.execute_reply.started":"2023-08-29T14:22:09.036324Z","shell.execute_reply":"2023-08-29T14:22:09.045551Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Feature Creation ##\nWe will create features through these 4 main ways:\n- **Manual creation** with the use of encoding, mathematical transforms, interactions, counts, break downs, group transforms, etc.\n- **K-means clustering**, which will help us break up complex relationships into more simple chunks for the model to understand.\n- **PCA (principal component analisis)**, which will help us create automathically new features based on linear combinations of the existing ones.\n- **Target encoding**, which will encode some features into some number derived directly from the target","metadata":{}},{"cell_type":"markdown","source":"### Manual feature creation ###\nThere are several ways to do this, we'll take an example of some work on the bonus lesson on the feature engineering course, which works on the \"house-prices-advanced-regression-technique\" dataset).","metadata":{}},{"cell_type":"code","source":"def mathematical_transforms(df):\n    X = pd.DataFrame()  # dataframe to hold new features\n    X[\"LivLotRatio\"] = df.GrLivArea / df.LotArea\n    X[\"Spaciousness\"] = (df.FirstFlrSF + df.SecondFlrSF) / df.TotRmsAbvGrd\n    # This feature ended up not helping performance\n    # X[\"TotalOutsideSF\"] = \\\n    #     df.WoodDeckSF + df.OpenPorchSF + df.EnclosedPorch + \\\n    #     df.Threeseasonporch + df.ScreenPorch\n    return X\n\n\ndef interactions(df):\n    X = pd.get_dummies(df.BldgType, prefix=\"Bldg\")\n    X = X.mul(df.GrLivArea, axis=0) # This multiplies X with df.GrLivArea\n    return X\n\n\ndef counts(df):\n    X = pd.DataFrame()\n    X[\"PorchTypes\"] = df[[\n        \"WoodDeckSF\",\n        \"OpenPorchSF\",\n        \"EnclosedPorch\",\n        \"Threeseasonporch\",\n        \"ScreenPorch\",\n    ]].gt(0.0).sum(axis=1) # .gt indicates that the columns have greater value than 0.0\n    return X\n\n\ndef break_down(df):\n    X = pd.DataFrame()\n    X[\"MSClass\"] = df.MSSubClass.str.split(\"_\", n=1, expand=True)[0]\n    return X\n\n\ndef group_transforms(df):\n    X = pd.DataFrame()\n    X[\"MedNhbdArea\"] = df.groupby(\"Neighborhood\")[\"GrLivArea\"].transform(\"median\")\n    return X","metadata":{"execution":{"iopub.status.busy":"2023-08-29T14:22:11.218312Z","iopub.execute_input":"2023-08-29T14:22:11.218743Z","iopub.status.idle":"2023-08-29T14:22:11.230722Z","shell.execute_reply.started":"2023-08-29T14:22:11.218708Z","shell.execute_reply":"2023-08-29T14:22:11.229350Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### K-Means Clustering ###\n\nThe first unsupervised algorithm we'll use to create features was k-means clustering. We can either use cluster labels as a feature (a column with `0, 1, 2, ...`) or you could use the *distance* of the observations to each cluster. These features can sometimes be effective at untangling complicated spatial relationships.","metadata":{}},{"cell_type":"code","source":"# There are some functions to apply those two ways of k-means clustering\n\ncluster_features = []\n\ndef cluster_labels(df, features, n_clusters=20):\n    X = df.copy()\n    X_scaled = X.loc[:, features]\n    X_scaled = (X_scaled - X_scaled.mean(axis=0)) / X_scaled.std(axis=0)\n    kmeans = KMeans(n_clusters=n_clusters, n_init=50, random_state=0)\n    X_new = pd.DataFrame()\n    X_new[\"Cluster\"] = kmeans.fit_predict(X_scaled)\n    return X_new\n\n\ndef cluster_distance(df, features, n_clusters=20):\n    X = df.copy()\n    X_scaled = X.loc[:, features]\n    X_scaled = (X_scaled - X_scaled.mean(axis=0)) / X_scaled.std(axis=0)\n    kmeans = KMeans(n_clusters=20, n_init=50, random_state=0)\n    X_cd = kmeans.fit_transform(X_scaled)\n    # Label features and join to dataset\n    X_cd = pd.DataFrame(\n        X_cd, columns=[f\"Centroid_{i}\" for i in range(X_cd.shape[1])]\n    )\n    return X_cd","metadata":{"execution":{"iopub.status.busy":"2023-08-29T14:22:13.959664Z","iopub.execute_input":"2023-08-29T14:22:13.960577Z","iopub.status.idle":"2023-08-29T14:22:13.984009Z","shell.execute_reply.started":"2023-08-29T14:22:13.960505Z","shell.execute_reply":"2023-08-29T14:22:13.982236Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Principal Component Analisis ###\nThis will be the second unsupervised algorithm we'll use to create features, it can be used to decompose the variational structure in the data. The PCA algorithm will give us *loadings* which described each component of variation, and also the *components* which were the transformed datapoints. The loadings can suggest features to create and the components we can use as features directly.\n\nThere are a few things to keep in mind when applying PCA:\n- PCA only works with numeric features, like continuous quantities or counts.\n- PCA is sensitive to scale. It's good practice to standardize your data before applying PCA, unless you know you have good reason not to.\n- Consider removing or constraining outliers, since they can have an undue influence on the results.\n\nThese are only a couple ways you could use the principal components. You could also try clustering using one or more components. One thing to note is that PCA doesn't change the distance between points -- it's just like a rotation. So clustering with the full set of components is the same as clustering with the original features. Instead, pick some subset of components, maybe those with the most variance or the highest MI scores.","metadata":{}},{"cell_type":"code","source":"# These are two functions to apply PCA\n\ndef apply_pca(X, standardize=True):\n    # Standardize\n    if standardize:\n        X = (X - X.mean(axis=0)) / X.std(axis=0)\n    # Create principal components\n    pca = PCA()\n    X_pca = pca.fit_transform(X)\n    # Convert to dataframe\n    component_names = [f\"PC{i+1}\" for i in range(X_pca.shape[1])]\n    X_pca = pd.DataFrame(X_pca, columns=component_names)\n    # Create loadings\n    loadings = pd.DataFrame(\n        pca.components_.T,  # transpose the matrix of loadings\n        columns=component_names,  # so the columns are the principal components\n        index=X.columns,  # and the rows are the original features\n    )\n    return pca, X_pca, loadings\n\n\ndef plot_variance(pca, width=8, dpi=100):\n    # Create figure\n    fig, axs = plt.subplots(1, 2)\n    n = pca.n_components_\n    grid = np.arange(1, n + 1)\n    # Explained variance\n    evr = pca.explained_variance_ratio_\n    axs[0].bar(grid, evr)\n    axs[0].set(\n        xlabel=\"Component\", title=\"% Explained Variance\", ylim=(0.0, 1.0)\n    )\n    # Cumulative Variance\n    cv = np.cumsum(evr)\n    axs[1].plot(np.r_[0, grid], np.r_[0, cv], \"o-\")\n    axs[1].set(\n        xlabel=\"Component\", title=\"% Cumulative Variance\", ylim=(0.0, 1.0)\n    )\n    # Set up figure\n    fig.set(figwidth=8, dpi=100)\n    return axs\n\n# This function can help when plotting the correlation between features\n\ndef corrplot(df, method=\"pearson\", annot=True, **kwargs):\n    sns.clustermap(\n        df.corr(method),\n        vmin=-1.0,\n        vmax=1.0,\n        cmap=\"icefire\",\n        method=\"complete\",\n        annot=annot,\n        **kwargs,\n    )\n\n\ncorrplot(df_train, annot=None)","metadata":{"execution":{"iopub.status.busy":"2023-08-29T14:22:15.965948Z","iopub.execute_input":"2023-08-29T14:22:15.966957Z","iopub.status.idle":"2023-08-29T14:22:16.035241Z","shell.execute_reply.started":"2023-08-29T14:22:15.966912Z","shell.execute_reply":"2023-08-29T14:22:16.033512Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### PCA Application - Indicate Outliers ####\nPCA can be useful to detect outliers, using categorical plots (`sns.catplot`) with the components extracted from pca to detect the outliers and looking for a pattern between them can be beneficial. At the same time, it's important to understand our data and interpret what our components are indicating based on their loadings.","metadata":{}},{"cell_type":"markdown","source":"### Target Encoding ###\nThis is the process which we'll encode some categorical features using some value derived from the target. This will work with categorical features which have a high number of unique values. One way is to use the m-estimate encoder, which encodes the feature with a weighted sum between the mean of the target for all elements of the dataframe and the mean of the target for all the elements of the same group (relative to the feature we're encoding) of the element we are encoding. Note that we will use a similar technique to cross-validation to fit the encoder and transform all the data.","metadata":{}},{"cell_type":"code","source":"# This is the class definition of our encoder, which contains the necessary functions to use it on target encoding\n\nclass CrossFoldEncoder:\n    def __init__(self, encoder, **kwargs):\n        self.encoder_ = encoder\n        self.kwargs_ = kwargs  # keyword arguments for the encoder\n        self.cv_ = KFold(n_splits=5)\n\n    # Fit an encoder on one split and transform the feature on the\n    # other. Iterating over the splits in all folds gives a complete\n    # transformation. We also now have one trained encoder on each\n    # fold.\n    def fit_transform(self, X, y, cols):\n        self.fitted_encoders_ = []\n        self.cols_ = cols\n        X_encoded = []\n        for idx_encode, idx_train in self.cv_.split(X):\n            fitted_encoder = self.encoder_(cols=cols, **self.kwargs_)\n            fitted_encoder.fit(\n                X.iloc[idx_encode, :], y.iloc[idx_encode],\n            )\n            X_encoded.append(fitted_encoder.transform(X.iloc[idx_train, :])[cols])\n            self.fitted_encoders_.append(fitted_encoder)\n        X_encoded = pd.concat(X_encoded)\n        X_encoded.columns = [name + \"_encoded\" for name in X_encoded.columns]\n        return X_encoded\n\n    # To transform the test data, average the encodings learned from\n    # each fold.\n    def transform(self, X):\n        from functools import reduce\n\n        X_encoded_list = []\n        for fitted_encoder in self.fitted_encoders_:\n            X_encoded = fitted_encoder.transform(X)\n            X_encoded_list.append(X_encoded[self.cols_])\n        X_encoded = reduce(\n            lambda x, y: x.add(y, fill_value=0), X_encoded_list\n        ) / len(X_encoded_list)\n        X_encoded.columns = [name + \"_encoded\" for name in X_encoded.columns]\n        return X_encoded","metadata":{"execution":{"iopub.status.busy":"2023-08-29T14:22:24.770284Z","iopub.execute_input":"2023-08-29T14:22:24.770684Z","iopub.status.idle":"2023-08-29T14:22:24.784194Z","shell.execute_reply.started":"2023-08-29T14:22:24.770652Z","shell.execute_reply":"2023-08-29T14:22:24.783135Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Scoring Dataset #\nNow, we just have to put all together in the same function to create all the new features and test the new dataset to see if is more useful that the one we had initially. Note that first of all is important to get a baseline score of the initial dataset to have a reference from where we'll evaluate our changes on new datasets.","metadata":{}},{"cell_type":"code","source":"def score_dataset(X, y, model=XGBRegressor()):\n    # Label encoding for categoricals\n    #\n    # Label encoding is good for XGBoost and RandomForest, but one-hot\n    # would be better for models like Lasso or Ridge. The `cat.codes`\n    # attribute holds the category levels.\n    for colname in X.select_dtypes([\"category\"]):\n        X[colname] = X[colname].cat.codes\n    # Metric for Housing competition is RMSLE (Root Mean Squared Log Error)\n    log_y = np.log(y)\n    score = cross_val_score(\n        model, X, log_y, cv=5, scoring=\"neg_mean_squared_error\",\n    )\n    score = -1 * score.mean()\n    score = np.sqrt(score)\n    return score\n\n# A function to create features variables can look\n# like this (this is an example from the bonus lesson\n# on the feature engineering course, which works on the\n# \"house-prices-advanced-regression-technique\" dataset).\n\ndef create_features(df, df_test=None):\n    X = df.copy()\n    y = X.pop(\"SalePrice\")\n    mi_scores = make_mi_scores(X, y)\n\n    # Combine splits if test data is given\n    #\n    # If we're creating features for test set predictions, we should\n    # use all the data we have available. After creating our features,\n    # we'll recreate the splits.\n    if df_test is not None:\n        X_test = df_test.copy()\n        X_test.pop(\"SalePrice\")\n        X = pd.concat([X, X_test])\n\n    # Lesson 2 - Mutual Information\n    X = drop_uninformative(X, mi_scores)\n\n    # Lesson 3 - Transformations\n    X = X.join(mathematical_transforms(X))\n    X = X.join(interactions(X))\n    X = X.join(counts(X))\n    # X = X.join(break_down(X))\n    X = X.join(group_transforms(X))\n\n    # Lesson 4 - Clustering\n    # X = X.join(cluster_labels(X, cluster_features, n_clusters=20))\n    # X = X.join(cluster_distance(X, cluster_features, n_clusters=20))\n\n    # Lesson 5 - PCA\n    X = X.join(pca_inspired(X))\n    # X = X.join(pca_components(X, pca_features))\n    # X = X.join(indicate_outliers(X))\n\n    X = label_encode(X)\n\n    # Reform splits\n    if df_test is not None:\n        X_test = X.loc[df_test.index, :]\n        X.drop(df_test.index, inplace=True)\n\n    # Lesson 6 - Target Encoder\n    encoder = CrossFoldEncoder(MEstimateEncoder, m=1)\n    X = X.join(encoder.fit_transform(X, y, cols=[\"MSSubClass\"]))\n    if df_test is not None:\n        X_test = X_test.join(encoder.transform(X_test))\n\n    if df_test is not None:\n        return X, X_test\n    else:\n        return X\n\n\ndf_train, df_test = load_data()\nX_train = create_features(df_train)\ny_train = df_train.loc[:, \"SalePrice\"]\n\nscore_dataset(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2023-08-29T14:22:27.033471Z","iopub.execute_input":"2023-08-29T14:22:27.034327Z","iopub.status.idle":"2023-08-29T14:22:27.129750Z","shell.execute_reply.started":"2023-08-29T14:22:27.034291Z","shell.execute_reply":"2023-08-29T14:22:27.127945Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Hyperparameter tunning #\nAt this stage, you might like to do some hyperparameter tuning with XGBoost before creating your final submission. You can do it by hand or you could try using one of scikit-learn's automatic [hyperparameter tuners](https://scikit-learn.org/stable/modules/grid_search.html). Or you could explore more advanced tuning libraries like [Optuna](https://optuna.readthedocs.io/en/stable/index.html) or [scikit-optimize](https://scikit-optimize.github.io/stable/). ","metadata":{}},{"cell_type":"code","source":"X_train = create_features(df_train)\ny_train = df_train.loc[:, \"SalePrice\"]\n\n# Here are some of the most important hyperparameters of xgb\nxgb_params = dict(\n    max_depth=6,           # maximum depth of each tree - try 2 to 10\n    learning_rate=0.01,    # effect of each tree - try 0.0001 to 0.1\n    n_estimators=1000,     # number of trees (that is, boosting rounds) - try 1000 to 8000\n    min_child_weight=1,    # minimum number of houses in a leaf - try 1 to 10\n    colsample_bytree=0.7,  # fraction of features (columns) per tree - try 0.2 to 1.0\n    subsample=0.7,         # fraction of instances (rows) per tree - try 0.2 to 1.0\n    reg_alpha=0.5,         # L1 regularization (like LASSO) - try 0.0 to 10.0\n    reg_lambda=1.0,        # L2 regularization (like Ridge) - try 0.0 to 10.0\n    num_parallel_tree=1,   # set > 1 for boosted random forests\n)\n\nxgb = XGBRegressor(**xgb_params)\nscore_dataset(X_train, y_train, xgb)","metadata":{"execution":{"iopub.status.busy":"2023-08-29T14:25:42.218028Z","iopub.execute_input":"2023-08-29T14:25:42.218496Z","iopub.status.idle":"2023-08-29T14:25:42.274715Z","shell.execute_reply.started":"2023-08-29T14:25:42.218463Z","shell.execute_reply":"2023-08-29T14:25:42.272667Z"},"trusted":true},"execution_count":null,"outputs":[]}]}